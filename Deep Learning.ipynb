{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c395336b-ad97-4684-b78a-ee8c5bca0e01",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "The following sample codes are examples of CNN, LSTM and a combined network with DNN on static features and LSTM on time series features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b32f0-485b-4b8c-96c2-8a757315b3aa",
   "metadata": {},
   "source": [
    "## 1. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd616c1-72fa-4c4d-9481-fd6f1c8b3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data to np array, reshape to 4 dimensional.\n",
    "from sklearn import preprocessing\n",
    "ActiveTran2 = preprocessing.normalize(ActiveTran1) #range 0-1\n",
    "ActiveTran2 = pd.DataFrame(ActiveTran2, columns = ActiveTran1.columns)\n",
    "ActiveTran3 = ActiveTran2.values\n",
    "\n",
    "tran = []\n",
    "for i in range(len(TrainLab)):\n",
    "    age = ActiveTran2.loc[i:i+5,['Age']]\n",
    "    tm = ActiveTran2.loc[i:i+5,['TenureMonth']]\n",
    "    s = ActiveTran2.loc[i:i + 5, ['NoOfSavingsP']]\n",
    "    l = ActiveTran2.loc[i:i + 5, ['NoOfLoans']]\n",
    "    dd = ActiveTran2.loc[i:i + 5, ['NumberofDirectDeposits']]\n",
    "    sd = ActiveTran2.loc[i:i + 5, ['SumofDirectDeposits']]\n",
    "    bp = ActiveTran2.loc[i:i + 5, ['NumberofBillPayTransactions']]\n",
    "    dc = ActiveTran2.loc[i:i + 5, ['NumberofDebitCardTransactions']]\n",
    "    ib = ActiveTran2.loc[i:i + 5, ['NumberofTransactionsConductedinBranch']]\n",
    "    fc = ActiveTran2.loc[i:i + 5, ['FEESCHARGED']]\n",
    "    sf = ActiveTran2.loc[i:i + 5, ['SUMofFEESCHARGED']]\n",
    "    tt = ActiveTran2.loc[i:i + 5, ['NumberofTotalTransactions']]\n",
    "    ol = ActiveTran2.loc[i:i + 5, ['OnlineTran']]\n",
    "    py = ActiveTran2.loc[i:i + 5, ['NumberofPayments']]\n",
    "    sp = ActiveTran2.loc[i:i + 5, ['SumofPaymentAmount']]\n",
    "    at = ActiveTran2.loc[i:i + 5, ['AllTran']]\n",
    "    a = np.vstack((age, tm, s, l, dd, sd, bp, dc, ib, fc, sf, tt, ol, py, sp, at))\n",
    "    a = a.tolist()\n",
    "    tran.append(a)\n",
    "TrainTran = np.array(tran)\n",
    "TrainTran = np.reshape(tran,(TrainTran.shape[0],16,6,1))\n",
    "\n",
    "#Convert Target to categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y = TrainLab[['Loyalty']].values\n",
    "Y = to_categorical(Y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41077742-2ac5-45d6-be92-d4f6f663c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "#K.set_image_data_format('channels_last')\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "#Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', input_shape=(16, 6, 1)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D((1,1), strides=(1,1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model, use cost sensitive learning\n",
    "model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer='adam', metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n",
    "\n",
    "\n",
    "#Fit model on training data\n",
    "model.fit(TrainTran1, Train_Y3a, batch_size=32, epochs = 100, verbose=2)\n",
    "\n",
    "#Evaluate model on test data\n",
    "score = model.evaluate(TestTran1, Test_Y, verbose=2) #accuracy = 0.4685 (1 dense layer)\n",
    "#0.4302 (without dropout 0.5), 0.4691 (with dropout 0.5), Add more conv2d layers 0.4681, add dropout layer 0.4731\n",
    "#Change batch size no difference\n",
    "\n",
    "history = model.fit(TrainTran1, Y, validation_data=(TestTran1, Test_Y), batch_size=32, epochs=50, verbose=2)\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a7943-cc21-4ca8-adb7-f103361c6c3a",
   "metadata": {},
   "source": [
    "## 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ceaac7-3fcc-4153-b394-00a99ccd3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation, convert train data to 3 dimensional np array\n",
    "from sklearn import preprocessing\n",
    "ActiveTran2 = preprocessing.normalize(ActiveTran1) #range 0-1\n",
    "ActiveTran2 = pd.DataFrame(ActiveTran2, columns = ActiveTran1.columns)\n",
    "\n",
    "tran = []\n",
    "for i in range(len(TrainLab)):\n",
    "    age = ActiveTran2.loc[i:i+5,['Age']]\n",
    "    tm = ActiveTran2.loc[i:i+5,['TenureMonth']]\n",
    "    s = ActiveTran2.loc[i:i + 5, ['NoOfSavingsP']]\n",
    "    l = ActiveTran2.loc[i:i + 5, ['NoOfLoans']]\n",
    "    dd = ActiveTran2.loc[i:i + 5, ['NumberofDirectDeposits']]\n",
    "    sd = ActiveTran2.loc[i:i + 5, ['SumofDirectDeposits']]\n",
    "    bp = ActiveTran2.loc[i:i + 5, ['NumberofBillPayTransactions']]\n",
    "    dc = ActiveTran2.loc[i:i + 5, ['NumberofDebitCardTransactions']]\n",
    "    ib = ActiveTran2.loc[i:i + 5, ['NumberofTransactionsConductedinBranch']]\n",
    "    fc = ActiveTran2.loc[i:i + 5, ['FEESCHARGED']]\n",
    "    sf = ActiveTran2.loc[i:i + 5, ['SUMofFEESCHARGED']]\n",
    "    tt = ActiveTran2.loc[i:i + 5, ['NumberofTotalTransactions']]\n",
    "    ol = ActiveTran2.loc[i:i + 5, ['OnlineTran']]\n",
    "    py = ActiveTran2.loc[i:i + 5, ['NumberofPayments']]\n",
    "    sp = ActiveTran2.loc[i:i + 5, ['SumofPaymentAmount']]\n",
    "    at = ActiveTran2.loc[i:i + 5, ['AllTran']]\n",
    "    a = np.hstack((age, tm, s, l, dd, sd, bp, dc, ib, fc, sf, tt, ol, py, sp, at))\n",
    "    a = a.tolist()\n",
    "    tran.append(a)\n",
    "TrainTran = np.array(tran)\n",
    "TrainTranL = np.reshape(tran,(TrainTran.shape[0],6,16))\n",
    "\n",
    "#Convert Target to categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Test_Y3 = TestLab[['Loyalty1']].values\n",
    "Test_Y3 = to_categorical(Test_Y3,3)\n",
    "\n",
    "Train_Y3 = TrainLab[['Loyalty1']].values\n",
    "Train_Y3 = to_categorical(Train_Y3,3)\n",
    "Y_train = Train_Y3.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cbe0d-8b71-40f8-962d-ef299661db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building LSTM\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "\n",
    "n_timesteps, n_features, n_outputs = TrainTranL.shape[1], TrainTranL.shape[2], Train_Y3.shape[1]\n",
    "\n",
    "verbose, epochs, batch_size = 2, 200, 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "#model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(TrainTranL, Train_Y3, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "# evaluate model\n",
    "accuracy = model.evaluate(TestTranL, Test_Y3, batch_size=batch_size, verbose=2)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Result of 150 epochs similar result with 200 epoch\n",
    "rounded_predictions = model.predict_classes(TestTranL, batch_size=batch_size, verbose=verbose)\n",
    "rounded_labels=np.argmax(Test_Y3, axis=1)\n",
    "print(classification_report(rounded_labels, rounded_predictions))\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0e165-9034-4a87-875e-0468305fe563",
   "metadata": {},
   "source": [
    "## 3. Combined Network: DNN (static features) and LSTM (time series features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f223d55-6160-4d03-b430-bef46ce09331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "ActiveTran2 = scaler.fit_transform(ActiveTran1b)\n",
    "ActiveTran2 = pd.DataFrame(ActiveTran2, columns = ActiveTran1b.columns)\n",
    "\n",
    "tran = []\n",
    "for i in range(len(TrainSet3a)):\n",
    "    dd = ActiveTran2.loc[i:i + 5, ['NumberofDirectDeposits']]\n",
    "    sd = ActiveTran2.loc[i:i + 5, ['SumofDirectDeposits']]\n",
    "    bp = ActiveTran2.loc[i:i + 5, ['NumberofBillPayTransactions']]\n",
    "    dc = ActiveTran2.loc[i:i + 5, ['NumberofDebitCardTransactions']]\n",
    "    ib = ActiveTran2.loc[i:i + 5, ['NumberofTransactionsConductedinBranch']]\n",
    "    fc = ActiveTran2.loc[i:i + 5, ['FEESCHARGED']]\n",
    "    sf = ActiveTran2.loc[i:i + 5, ['SUMofFEESCHARGED']]\n",
    "    tt = ActiveTran2.loc[i:i + 5, ['NumberofTotalTransactions']]\n",
    "    ol = ActiveTran2.loc[i:i + 5, ['OnlineTran']]\n",
    "    py = ActiveTran2.loc[i:i + 5, ['NumberofPayments']]\n",
    "    sp = ActiveTran2.loc[i:i + 5, ['SumofPaymentAmount']]\n",
    "    at = ActiveTran2.loc[i:i + 5, ['AllTran']]\n",
    "    a = np.hstack((dd, sd, bp, dc, ib, fc, sf, tt, ol, py, sp, at))\n",
    "    a = a.tolist()\n",
    "    tran.append(a)\n",
    "TrainTS = np.array(tran)\n",
    "\n",
    "#Convert Target to categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Test_Y3 = TestLab[['Loyalty1']].values\n",
    "Test_Y3 = to_categorical(Test_Y3,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36563662-1a4b-4306-987a-43cc1ed8c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM for time series inputs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "n_timesteps, n_features = TrainTS.shape[1], TrainTS.shape[2]\n",
    "X1 = TrainTS\n",
    "\n",
    "#First input model\n",
    "visible1 = Input(shape=(n_timesteps, n_features))\n",
    "LSTM1 = LSTM(48, activation='relu')(visible1)\n",
    "LSTM1 = Dropout(0.5)(LSTM1)\n",
    "LSTM1 = Flatten()(LSTM1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f52775f-8027-49f4-912f-a7e62b31b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare static predictors\n",
    "Train = Train_XML.values\n",
    "Test = Test_XML.values\n",
    "\n",
    "#Second input model\n",
    "X2 = Train\n",
    "\n",
    "visible2 = Input(shape=Train.shape[1])\n",
    "DNN1 = Dense(72, activation='relu')(visible2)\n",
    "DNN1 = Dropout(0.5)(DNN1)\n",
    "DNN1 = Dense(24, activation='relu')(DNN1)\n",
    "DNN1 = Flatten()(DNN1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d56cc-47b8-4595-93ea-d2fdce36536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge input models\n",
    "n_outputs = Train_Y3.shape[1]\n",
    "\n",
    "Merge1 = concatenate([LSTM1, DNN1])\n",
    "DNN2 = Dense(50, activation='relu')(Merge1)\n",
    "DNN2 = Dropout(0.5)(DNN2)\n",
    "DNN2 = Dense(25, activation='relu')(DNN2)\n",
    "DNN2 = Dropout(0.5)(DNN2)\n",
    "output = Dense(n_outputs, activation='softmax')(DNN2)\n",
    "model = Model(inputs=[visible1, visible2], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n",
    "\n",
    "history = model.fit([X1, X2], Train_Y3, epochs=125, verbose=2)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Result of 100 epochs\n",
    "verbose, batch_size = 2, 32\n",
    "rounded_predictions = model.predict([TestTS, Test], batch_size=batch_size, verbose=verbose)\n",
    "prediction = np.argmax(rounded_predictions, axis=1)\n",
    "rounded_labels=np.argmax(Test_Y3, axis=1)\n",
    "print(classification_report(rounded_labels, prediction, digits=4))\n",
    "\n",
    "# Visualize loss history\n",
    "training_loss = history.history['loss']\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f2916-8d86-4e85-b2b5-208273c43405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
